{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4.13. Creating Data Processing Pipelines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Problem\n",
      "You want to process data iteratively in the style of a data processing pipeline (similar to\n",
      "Unix pipes). For instance, you have a huge amount of data that needs to be processed,\n",
      "but it can\u2019t fit entirely into memory.\n",
      "\n",
      "# Solution\n",
      "Generator functions are a good way to implement processing pipelines. To illustrate,\n",
      "suppose you have a huge directory of log files that you want to process:\n",
      "```\n",
      "foo/\n",
      "    access-log-012007.gz\n",
      "    access-log-022007.gz\n",
      "    access-log-032007.gz\n",
      "    \u2026\n",
      "    access-log-012008\n",
      "bar/\n",
      "    access-log-092007.bz2\n",
      "    \u2026\n",
      "    access-log-022008\n",
      "```\n",
      "Suppose each file contains lines of data like this:\n",
      "```\n",
      "124.115.6.12 - - [10/Jul/2012:00:18:50 -0500] \"GET /robots.txt ...\" 200 71\n",
      "210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /ply/ ...\" 200 11875\n",
      "210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] \"GET /favicon.ico ...\" 404 369\n",
      "61.135.216.105 - - [10/Jul/2012:00:20:04 -0500] \"GET /blog/atom.xml ...\" 304 -\n",
      "\u2026\n",
      "```\n",
      "To process these files, you could define a collection of small generator functions that\n",
      "perform specific self-contained tasks. For example:\n",
      "```python\n",
      "import os\n",
      "import fnmatch\n",
      "import gzip\n",
      "import bz2\n",
      "import re\n",
      "\n",
      "def gen_find(filepat, top):\n",
      "    '''\n",
      "    Find all filenames in a directory tree that match a shell wildcard pattern\n",
      "    '''\n",
      "    for path, dirlist, filelist in os.walk(top):\n",
      "        for name in fnmatch.filter(filelist, filepat):\n",
      "            yield os.path.join(path,name)\n",
      "    \n",
      "def gen_opener(filenames):\n",
      "    '''\n",
      "    Open a sequence of filenames one at a time producing a file object.\n",
      "    The file is closed immediately when proceeding to the next iteration.\n",
      "    '''\n",
      "    for filename in filenames:\n",
      "        if filename.endswith('.gz'):\n",
      "            f = gzip.open(filename, 'rt')\n",
      "        elif filename.endswith('.bz2'):\n",
      "            f = bz2.open(filename, 'rt')\n",
      "        else:\n",
      "            f = open(filename, 'rt')\n",
      "        yield f\n",
      "        f.close()\n",
      "\n",
      "def gen_concatenate(iterators):\n",
      "    '''\n",
      "    Chain a sequence of iterators together into a single sequence.\n",
      "    '''\n",
      "    for it in iterators:\n",
      "        yield from it\n",
      "\n",
      "def gen_grep(pattern, lines):\n",
      "    '''\n",
      "    Look for a regex pattern in a sequence of lines\n",
      "    '''\n",
      "    pat = re.compile(pattern)\n",
      "    for line in lines:\n",
      "        if pat.search(line):\n",
      "            yield line\n",
      "```\n",
      "You can now easily stack these functions together to make a processing pipeline. For\n",
      "example, to find all log lines that contain the word _python_, you would just do this:\n",
      "```python\n",
      "lognames = gen_find('access-log*', 'www')\n",
      "files = gen_opener(lognames)\n",
      "lines = gen_concatenate(files)\n",
      "pylines = gen_grep('(?i)python', lines)\n",
      "for line in pylines:\n",
      "    print(line)\n",
      "```\n",
      "If you want to extend the pipeline further, you can even feed the data in generator\n",
      "expressions. For example, this version finds the number of bytes transferred and sums\n",
      "the total:\n",
      "```python\n",
      "lognames = gen_find('access-log*', 'www')\n",
      "files = gen_opener(lognames)\n",
      "lines = gen_concatenate(files)\n",
      "pylines = gen_grep('(?i)python', lines)\n",
      "bytecolumn = (line.rsplit(None,1)[1] for line in pylines)\n",
      "bytes = (int(x) for x in bytecolumn if x != '-')\n",
      "print('Total', sum(bytes))\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Discussion\n",
      "Processing data in a pipelined manner works well for a wide variety of other problems,\n",
      "including parsing, reading from real-time data sources, periodic polling, and so on.\n",
      "\n",
      "In understanding the code, it is important to grasp that the `yield` statement acts as a\n",
      "kind of data producer whereas a `for` loop acts as a data consumer. When the generators\n",
      "are stacked together, each `yield` feeds a single item of data to the next stage of the\n",
      "pipeline that is consuming it with iteration. In the last example, the `sum()` function is\n",
      "actually driving the entire program, pulling one item at a time out of the pipeline of\n",
      "generators.\n",
      "\n",
      "One nice feature of this approach is that each generator function tends to be small and\n",
      "self-contained. As such, they are easy to write and maintain. In many cases, they are so\n",
      "general purpose that they can be reused in other contexts. The resulting code that glues\n",
      "the components together also tends to read like a simple recipe that is easily understood.\n",
      "\n",
      "The memory efficiency of this approach can also not be overstated. The code shown\n",
      "would still work even if used on a massive directory of files. In fact, due to the iterative\n",
      "nature of the processing, very little memory would be used at all.\n",
      "\n",
      "There is a bit of extreme subtlety involving the `gen_concatenate()` function. The\n",
      "`purpose` of this function is to concatenate input sequences together into one long sequence \n",
      "of lines. The `itertools.chain()` function performs a similar function, but requires \n",
      "that all of the chained iterables be specified as arguments. In the case of this\n",
      "particular recipe, doing that would involve a statement such as `lines = itertools.chain(*files)`, \n",
      "which would cause the `gen_opener()` generator to be fully consumed. \n",
      "Since that generator is producing a sequence of open files that are immediately\n",
      "closed in the next iteration step, `chain()` can\u2019t be used. The solution shown avoids this\n",
      "issue.\n",
      "\n",
      "Also appearing in the `gen_concatenate()` function is the use of `yield from` to delegate\n",
      "to a subgenerator. The statement `yield from it` simply makes `gen_concatenate()`\n",
      "emit all of the values produced by the generator `it`. This is described further in\n",
      "Recipe 4.14.\n",
      "\n",
      "Last, but not least, it should be noted that a pipelined approach doesn\u2019t always work for\n",
      "every data handling problem. Sometimes you just need to work with all of the data at\n",
      "once. However, even in that case, using generator pipelines can be a way to logically\n",
      "break a problem down into a kind of workflow.\n",
      "\n",
      "David Beazley has written extensively about these techniques in his [\u201cGenerator Tricks\n",
      "for Systems Programmers\u201d](http://www.dabeaz.com/generators/) tutorial presentation. Consult that for even more examples."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}