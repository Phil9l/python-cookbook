{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2.18. Tokenizing Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Problem\n",
      "You have a string that you want to parse left to right into a stream of tokens.\n",
      "\n",
      "# Solution\n",
      "Suppose you have a string of text such as this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = 'foo = 23 + 42 * 10'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To tokenize the string, you need to do more than merely match patterns. You need to\n",
      "have some way to identify the kind of pattern as well. For instance, you might want to\n",
      "turn the string into a sequence of pairs like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = [('NAME', 'foo'), ('EQ','='), ('NUM', '23'), ('PLUS','+'),\n",
      "          ('NUM', '42'), ('TIMES', '*'), ('NUM', '10')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To do this kind of splitting, the first step is to define all of the possible tokens, including\n",
      "whitespace, by regular expression patterns using named capture groups such as this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\n",
      "NUM = r'(?P<NUM>\\d+)'\n",
      "PLUS = r'(?P<PLUS>\\+)'\n",
      "TIMES = r'(?P<TIMES>\\*)'\n",
      "EQ = r'(?P<EQ>=)'\n",
      "WS = r'(?P<WS>\\s+)'\n",
      "\n",
      "master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In these `re` patterns, the `?P<TOKENNAME>` convention is used to assign a name to the\n",
      "pattern. This will be used later.\n",
      "\n",
      "Next, to tokenize, use the little-known `scanner()` method of pattern objects. This\n",
      "method creates a scanner object in which repeated calls to `match()` step through the\n",
      "supplied text one match at a time. Here is an interactive example of how a scanner object\n",
      "works:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner = master_pat.scanner('foo = 42')\n",
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<_sre.SRE_Match object; span=(0, 3), match='foo'>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_.lastgroup, _.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "('NAME', 'foo')"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<_sre.SRE_Match object; span=(3, 4), match=' '>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_.lastgroup, _.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "('WS', ' ')"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<_sre.SRE_Match object; span=(4, 5), match='='>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_.lastgroup, _.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "('EQ', '=')"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<_sre.SRE_Match object; span=(5, 6), match=' '>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_.lastgroup, _.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "('WS', ' ')"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<_sre.SRE_Match object; span=(6, 8), match='42'>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_.lastgroup, _.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "('NUM', '42')"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scanner.match()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To take this technique and put it into code, it can be cleaned up and easily packaged\n",
      "into a generator like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "\n",
      "Token = namedtuple('Token', ['type','value'])\n",
      "\n",
      "def generate_tokens(pat, text):\n",
      "    scanner = pat.scanner(text)\n",
      "    for m in iter(scanner.match, None):\n",
      "        yield Token(m.lastgroup, m.group())\n",
      "\n",
      "# Example use\n",
      "for tok in generate_tokens(master_pat, 'foo = 42'):\n",
      "    print(tok)\n",
      "\n",
      "# Produces output\n",
      "# Token(type='NAME', value='foo')\n",
      "# Token(type='WS', value=' ')\n",
      "# Token(type='EQ', value='=')\n",
      "# Token(type='WS', value=' ')\n",
      "# Token(type='NUM', value='42')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Token(type='NAME', value='foo')\n",
        "Token(type='WS', value=' ')\n",
        "Token(type='EQ', value='=')\n",
        "Token(type='WS', value=' ')\n",
        "Token(type='NUM', value='42')\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to filter the token stream in some way, you can either define more generator\n",
      "functions or use a generator expression. For example, here is how you might filter out\n",
      "all whitespace tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = (tok for tok in generate_tokens(master_pat, text)\n",
      "          if tok.type != 'WS')\n",
      "for tok in tokens:\n",
      "    print(tok)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Token(type='NAME', value='foo')\n",
        "Token(type='EQ', value='=')\n",
        "Token(type='NUM', value='23')\n",
        "Token(type='PLUS', value='+')\n",
        "Token(type='NUM', value='42')\n",
        "Token(type='TIMES', value='*')\n",
        "Token(type='NUM', value='10')\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Discussion\n",
      "Tokenizing is often the first step for more advanced kinds of text parsing and handling.\n",
      "To use the scanning technique shown, there are a few important details to keep in mind.\n",
      "First, you must make sure that you identify every possible text sequence that might\n",
      "appear in the input with a correponding `re` pattern. If any nonmatching text is found,\n",
      "scanning simply stops. This is why it was necessary to specify the whitespace (`WS`) token\n",
      "in the example.\n",
      "\n",
      "The order of tokens in the master regular expression also matters. When matching, re\n",
      "tries to match pattens in the order specified. Thus, if a pattern happens to be a substring\n",
      "of a longer pattern, you need to make sure the longer pattern goes first. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LT = r'(?P<LT><)'\n",
      "LE = r'(?P<LE><=)'\n",
      "EQ = r'(?P<EQ>=)'\n",
      "\n",
      "master_pat = re.compile('|'.join([LE, LT, EQ])) # Correct\n",
      "# master_pat = re.compile('|'.join([LT, LE, EQ])) # Incorrect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second pattern is wrong because it would match the text `<=` as the token `LT` followed\n",
      "by the token `EQ`, not the single token `LE`, as was probably desired.\n",
      "\n",
      "Last, but not least, you need to watch out for patterns that form substrings. For example,\n",
      "suppose you have two pattens like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PRINT = r'(P<PRINT>print)'\n",
      "NAME = r'(P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'\n",
      "\n",
      "master_pat = re.compile('|'.join([PRINT, NAME]))\n",
      "\n",
      "for tok in generate_tokens(master_pat, 'printer'):\n",
      "    print(tok)\n",
      "\n",
      "# Outputs :\n",
      "# Token(type='PRINT', value='print')\n",
      "# Token(type='NAME', value='er')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For more advanced kinds of tokenizing, you may want to check out packages such as\n",
      "PyParsing or PLY. An example involving PLY appears in the next recipe."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}